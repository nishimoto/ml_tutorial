{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習の精度を上げる（パラメーターチューニング）\n",
    "\n",
    "## 目次\n",
    "\n",
    " - ハイパーパラメーターとは？\n",
    " - グリッドサーチによるパラメーターサーチ\n",
    " - ベイズ最適化によるパラメーターサーチ\n",
    " - スタッキングによる精度向上\n",
    "\n",
    "## ハイパーパラメーターとは?\n",
    "\n",
    "「機械学習アルゴリズムにおいて、人が調整するべきパラメーターのこと」です。\n",
    "\n",
    "例えば、ディープラーニングにおける、層の数などがパラメーターに相当します。\n",
    "\n",
    "<img src=\"./assets/pic/deeplearning1.png\" width=\"50%\">\n",
    "\n",
    "今回は、2種類の手法でハイパーパラメータを探索します。\n",
    "\n",
    "①：グリッドサーチ（いくつかの値の全組み合わせを試し、最適な値を探す）\n",
    "\n",
    "②：ベイズ最適化によるパラメーターサーチ\n",
    "\n",
    "②の方が使用されていることは多いですが、両方試してみます。\n",
    "\n",
    "## 探索範囲について\n",
    "\n",
    "私が使用しているパラメーター探索範囲は以下です。[公式のリファレンス](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier)も参照ください。\n",
    "\n",
    "```python\n",
    "all_params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'min_child_weight': [3, 5, 10],\n",
    "    'n_estimetors': [10000],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'colsample_bylevel': [0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'random_state': [0],\n",
    "    'n_jobs': [1],\n",
    "}\n",
    "```\n",
    "\n",
    "## グリッドサーチによるパラメーターサーチ\n",
    "\n",
    "(`sklearn.model_selection.ParameterGrid`)[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html] に上記の辞書を与えると、全通りのパラメーターを作ってくれます。\n",
    "\n",
    "以下は検証用のミニコード。\n",
    "\n",
    " - ソースコード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 0.8, 'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimetors': 10000, 'n_jobs': 1, 'random_state': 0, 'reg_alpha': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "all_params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1],\n",
    "    'min_child_weight': [3, 5, 10],\n",
    "    'n_estimetors': [10000],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'colsample_bylevel': [0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'random_state': [0],\n",
    "    'n_jobs': [1],\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(all_params):\n",
    "    print(params)\n",
    "    break # ここコメントアウトすると全部のパラメーターが出力されていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際のソースコードで試してみます。\n",
    "\n",
    " - ソースコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8407a562277248cfa0679a3c80647091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8316498316498316 {'colsample_bylevel': 0.8, 'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 5, 'n_estimetors': 10000, 'n_jobs': 1, 'random_state': 0, 'reg_alpha': 0}\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 組み合わせが多いので、進捗を可視化するツールを入れました。\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# all_paramsはグローバル変数として宣言\n",
    "all_params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1],\n",
    "    'min_child_weight': [3, 5, 10],\n",
    "    'n_estimetors': [10000],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'colsample_bylevel': [0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'random_state': [0],\n",
    "    'n_jobs': [1],\n",
    "}\n",
    "\n",
    "\n",
    "def validate(train_x, train_y, params):\n",
    "    accuracies = []\n",
    "    feature_importances = []\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    for train_idx, test_idx in cv.split(train_x, train_y):\n",
    "        trn_x = train_x.iloc[train_idx, :]\n",
    "        val_x = train_x.iloc[test_idx, :]\n",
    "\n",
    "        trn_y = train_y.iloc[train_idx]\n",
    "        val_y = train_y.iloc[test_idx]\n",
    "\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "        clf.fit(trn_x, trn_y)\n",
    "\n",
    "        pred_y = clf.predict(val_x)\n",
    "        feature_importances.append(clf.feature_importances_)\n",
    "        accuracies.append(accuracy_score(val_y, pred_y))\n",
    "    return accuracies, feature_importances\n",
    "\n",
    "\n",
    "def plot_feature_importances(feature_importances, cols):\n",
    "    df_fimp = pd.DataFrame(feature_importances, columns=cols)\n",
    "    df_fimp.plot(kind=\"box\", rot=90)\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # CabinはこのあとDropするので、コードから削除\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode())\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "   \n",
    "    # 列の削除\n",
    "    df.drop([\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"], axis=1, inplace=True)\n",
    "\n",
    "    # Sexの01化とEmbarkedのダミー化 \n",
    "    df[\"Sex\"] = df[\"Sex\"].replace({\"male\": 0, \"female\": 1})\n",
    "    df = pd.get_dummies(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# test dataのpredict\n",
    "def predict_df(train_x, train_y, test_x, df_test_raw, path_output=\"result.csv\"):\n",
    "    params = {'learning_rate': 0.008306052798923729, 'max_depth': 7, 'min_child_weight': 3, 'colsample_bytree': 0.8210307463506532, 'colsample_bylevel': 0.8061816543590015}\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "    clf.fit(train_x, train_y)\n",
    "    preds = clf.predict(test_x)\n",
    "    \n",
    "    _df = pd.DataFrame()\n",
    "    _df[\"PassengerId\"] = df_test_raw[\"PassengerId\"]\n",
    "    _df[\"Survived\"] = preds\n",
    "    _df.to_csv(path_output, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    df_train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    # ここは前処理\n",
    "    train_y = df_train[\"Survived\"]\n",
    "    train_x = df_train.drop(\"Survived\", axis=1)\n",
    "\n",
    "    train_x = preprocess_df(train_x)\n",
    "    accuracies, feature_importances = validate(train_x, train_y, {})\n",
    "    plot_feature_importances(feature_importances, train_x.columns)\n",
    "\n",
    "    flag_product = True\n",
    "    if flag_product:\n",
    "        df_test = pd.read_csv(\"test.csv\")\n",
    "        df_test_raw = df_test.copy()\n",
    "        test_x = preprocess_df(df_test)\n",
    "        predict_df(train_x, train_y, test_x, df_test_raw, \"result.csv\")\n",
    "\n",
    "# main文を書き換えているので、別関数として定義\n",
    "def main_parametersearch():\n",
    "    df_train = pd.read_csv(\"train.csv\")\n",
    "    train_y = df_train[\"Survived\"]\n",
    "    train_x = df_train.drop(\"Survived\", axis=1)\n",
    "    train_x = preprocess_df(train_x)\n",
    "\n",
    "    # ここまではmainと同じ\n",
    "    # tqdmで囲むことで、進捗を可視化できます。\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    for params in tqdm(ParameterGrid(all_params)):\n",
    "        accuracies, feature_importances = validate(train_x, train_y, params)\n",
    "        \n",
    "        # もしaccuracyの平均値が最大だった場合、\n",
    "        # best_scoreを更新して、best_paramsを更新する。\n",
    "        if np.mean(accuracies) > best_score:\n",
    "            best_score = np.mean(accuracies)\n",
    "            best_params = params\n",
    "    print(best_score, best_params)\n",
    "\n",
    "# 呼んでいる関数を変えた\n",
    "if __name__ == '__main__':\n",
    "    main_parametersearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元々のaccuracyより0.01ほど精度が上がっているのがわかると思います。\n",
    "\n",
    "次に、ベイズ最適化によるパラメーターサーチを試してみます。\n",
    "\n",
    "## ベイズ最適化によるパラメーターサーチ\n",
    "\n",
    "ベイズ最適化はハイパーパラメーターをより効率的に探してくれるためのアルゴリズムです。\n",
    "\n",
    "原理については[明治大の金子先生のページ](https://datachemeng.com/bayesianoptimization/)がわかりやすいです。\n",
    "\n",
    "イメージとしては、 \n",
    "\n",
    "①：よい精度を出したところを深く探索する\n",
    "\n",
    "②：たまにハイパーパラメーターを全く変えて、もっと深いところがないか探索する\n",
    "\n",
    "の2つを組み合わせることで最適なパラメーターを探索しています。\n",
    "\n",
    "ここでは[PFNの人が作られたOptunaというライブラリー](https://optuna.org/)を使用します。\n",
    "\n",
    " -  ソースコード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16947250280583603\n",
      "{'max_depth': 4, 'min_child_weight': 7, 'colsample_bytree': 0.9780696556434307, 'colsample_bylevel': 0.9482914190554899}\n"
     ]
    }
   ],
   "source": [
    "# !pip install optuna # ライブラリーのインストールコマンド\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# optunaの出力をsupressする\n",
    "# https://optuna.readthedocs.io/en/stable/faq.html#how-to-suppress-log-messages-of-optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'seed': 0,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 3, 10),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.8, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_loguniform('colsample_bylevel', 0.8, 1.0),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    accuracies = []\n",
    "    for train_idx, test_idx in cv.split(train_x, train_y):\n",
    "        trn_x = train_x.iloc[train_idx, :]\n",
    "        val_x = train_x.iloc[test_idx, :]\n",
    "\n",
    "        trn_y = train_y.iloc[train_idx]\n",
    "        val_y = train_y.iloc[test_idx]\n",
    "\n",
    "        # main - Predict\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "        clf.fit(trn_x, trn_y)\n",
    "\n",
    "        pred_y = clf.predict(val_x)\n",
    "        accuracies.append(accuracy_score(val_y, pred_y))\n",
    "\n",
    "    return 1.0 - np.mean(accuracies)\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # CabinはこのあとDropするので、コードから削除\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode())\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "   \n",
    "    # 列の削除\n",
    "    df.drop([\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"], axis=1, inplace=True)\n",
    "\n",
    "    # Sexの01化とEmbarkedのダミー化 \n",
    "    df[\"Sex\"] = df[\"Sex\"].replace({\"male\": 0, \"female\": 1})\n",
    "    df = pd.get_dummies(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# main\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "train_y = df_train[\"Survived\"]\n",
    "train_x = df_train.drop(\"Survived\", axis=1)\n",
    "train_x = preprocess_df(train_x)\n",
    "\n",
    "# random_stateを固定する\n",
    "# 実際は要らないですが、今回はチュートリアルなので導入しています。\n",
    "# https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-obtain-reproducible-optimization-results\n",
    "sampler = optuna.samplers.TPESampler(seed=100) # Make the sampler behave in a deterministic way.\n",
    "study = optuna.create_study(sampler=sampler)\n",
    "study.optimize(objective, n_trials=100, n_jobs=1)\n",
    "print(study.best_trial.value)\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optunaは使い方が少し特殊です。\n",
    "\n",
    "基本的には\n",
    "\n",
    "①：精度を返す関数を作る （`objective`関数）\n",
    "\n",
    "②：その関数をOptunaに投げる　（`study.optimize`の引数に取る）\n",
    " \n",
    "の2工程で対応できます。\n",
    "\n",
    "詳細はソースコード + コメントをご参照ください。\n",
    "\n",
    "最後に、実際にtest.csvも予測してみます。\n",
    "\n",
    " - ソースコード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830527497194164\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEuCAYAAACedunCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAfAklEQVR4nO3de5QdZZ3u8e9DuI4iOhCVgUAQoxK5iLZ4XSp4OXAcQQWUjI7i6ETXEmXEUcIcBwYcHWCc8XgUl6KijKNy8TYRonC8X5E0yC0EjhFBIo4GRUQuksBz/qhqstPZ3b2T3vutvYvns1Yvd9Uu9vuzuvNU7bfeeku2iYiI0bdF0wVERER/JNAjIloigR4R0RIJ9IiIlkigR0S0RAI9IqIltmyq4Z122snz589vqvmIiJF02WWX3Wp7brf3Ggv0+fPnMz4+3lTzEREjSdJNU72XLpeIiJZIoEdEtERPgS7pYEnXS1olackU27xC0rWSVkj6bH/LjIiImczYhy5pDnAG8EJgNbBc0lLb13ZsswA4AXiW7dskPXJQBUdERHe9nKEfAKyyfYPte4FzgMMmbfO3wBm2bwOw/Zv+lhkRETPpJdB3AW7uWF5dr+v0OOBxkn4g6RJJB3f7IEmLJY1LGl+zZs3mVRwREV31Eujqsm7ynLtbAguA5wGLgI9LevhG/5F9pu0x22Nz53YdRhkREZupl0BfDczrWN4VuKXLNv9le63tnwPXUwV8REQU0kugLwcWSNpD0tbAUcDSSdt8GTgQQNJOVF0wN/Sz0GEhacafiIgmzBjottcBxwAXASuB82yvkHSKpEPrzS4CfivpWuBbwDts/3ZQRTfJ9gY/ux9/wUbrIiKaoKYCaGxszKNw6/9+J1/M7XevndVn7LDdVlx50ov6VFFEPJhJusz2WLf3GpvLZVTcfvdabjz1xbP6jPlLLuxTNRERU0ugz2D7vZawz9ldb47dhM8AmN1BISJiJgn0Gdyx8tScoUfESMjkXBERLZFAj4hoiQR6RERLJNAjIloigR4R0RIJ9IiIlkigR0S0RAI9IqIlEugRES2RQI+IaIkEekRESyTQIyJaIoEeEdESCfSIiJbI9Lk9mO30tztst1WfKomImFoCfQYzzYU+f8mFs54vPSKiH9LlEhHREgn0iIiWSKBHRLREAj0ioiUS6BERLdFToEs6WNL1klZJWtLl/aMlrZF0Rf3zhv6XGhER05lx2KKkOcAZwAuB1cBySUttXztp03NtHzOAGiMioge9nKEfAKyyfYPte4FzgMMGW1ZERGyqXgJ9F+DmjuXV9brJDpd0laTPS5rXl+oiIqJnvQS6uqzzpOWvAPNt7wt8HTi76wdJiyWNSxpfs2bNplUaERHT6iXQVwOdZ9y7Ard0bmD7t7b/VC9+DHhKtw+yfabtMdtjc+fO3Zx6IyJiCr0E+nJggaQ9JG0NHAUs7dxA0s4di4cCK/tXYkRE9GLGUS6210k6BrgImAOcZXuFpFOAcdtLgbdKOhRYB/wOOHqANUdERBc9zbZoexmwbNK6EztenwCc0N/SIiJiU2T63E0kbXyNWKdtuGxPvmYcETF4CfRNlLCOiGGVuVwiIloigR4R0RIJ9IiIlkigR0S0RAI9IqIlEugRES2RQI+IaIkEekRESyTQIyJaIoEeEdESCfSIiJZIoEdEtEQCPSKiJRLoEREtkUCPiGiJBHpEREsk0CMiWiKBHhHREgn0iIiWSKBHRLREAj0ioiUS6BERLdFToEs6WNL1klZJWjLNdkdIsqSx/pUYERG9mDHQJc0BzgAOARYCiyQt7LLd9sBbgR/3u8iIiJhZL2foBwCrbN9g+17gHOCwLtu9GzgduKeP9UVERI96CfRdgJs7llfX6x4gaX9gnu0L+lhbRERsgl4CXV3W+YE3pS2A9wNvn/GDpMWSxiWNr1mzpvcqIyJiRr0E+mpgXsfyrsAtHcvbA3sD35Z0I/B0YGm3C6O2z7Q9Znts7ty5m191RERspJdAXw4skLSHpK2Bo4ClE2/avt32Trbn254PXAIcant8IBVHRERXMwa67XXAMcBFwErgPNsrJJ0i6dBBFxgREb3ZspeNbC8Dlk1ad+IU2z5v9mVFRMSmyp2iEREtkUCPiGiJBHpEREsk0CMiWiKBHhHREgn0iIiWSKBHRLREAj0ioiUS6BERLZFAj4hoiQR6RERLJNAjIloigR4R0RIJ9IiIlkigR0S0RAI9IqIlEugRES2RQI+IaIkEekRESyTQIyJaIoEeEdESCfSIiJZIoEdEtEQCPSKiJRLoEREt0VOgSzpY0vWSVkla0uX9N0m6WtIVkr4vaWH/S42IiOnMGOiS5gBnAIcAC4FFXQL7s7b3sf0k4HTg3/teaURETKuXM/QDgFW2b7B9L3AOcFjnBrb/0LH4EMD9KzEiInqxZQ/b7ALc3LG8Gnja5I0kvRk4DtgaOKgv1UVERM96OUNXl3UbnYHbPsP2nsDxwLu6fpC0WNK4pPE1a9ZsWqURETGtXgJ9NTCvY3lX4JZptj8HeGm3N2yfaXvM9tjcuXN7rzIiImbUS6AvBxZI2kPS1sBRwNLODSQt6Fh8MfDT/pUYERG9mLEP3fY6SccAFwFzgLNsr5B0CjBueylwjKQXAGuB24DXDrLoiIjYWC8XRbG9DFg2ad2JHa+P7XNdERGxiXKnaERESyTQIyJaIoEeEdESCfSIiJZIoEdEtEQCPSKiJRLoEREtkUCPiGiJBHpEREsk0CMiWiKBHhHREgn0iIiWSKBHRLREAj0ioiUS6BERLZFAj4hoiQR6RERLJNAjIloigR4R0RIJ9IiIlkigR0S0RAI9IqIlEugRES2RQI+IaImeAl3SwZKul7RK0pIu7x8n6VpJV0n6hqTd+19qRERMZ8ZAlzQHOAM4BFgILJK0cNJmPwHGbO8LfB44vd+FRkTE9Ho5Qz8AWGX7Btv3AucAh3VuYPtbtu+qFy8Bdu1vmRERMZNeAn0X4OaO5dX1uqm8HvjqbIqKiIhNt2UP26jLOnfdUHo1MAY8d4r3FwOLAXbbbbceS4yIiF70coa+GpjXsbwrcMvkjSS9APhfwKG2/9Ttg2yfaXvM9tjcuXM3p96IiJhCL4G+HFggaQ9JWwNHAUs7N5C0P/BRqjD/Tf/LjIiImcwY6LbXAccAFwErgfNsr5B0iqRD683+FXgocL6kKyQtneLjIiJiQHrpQ8f2MmDZpHUndrx+QZ/rioiITdRToA8Lqdv12Q3ZXa/XRkS03kjd+m97g5/dj79go3UREQ9WIxXoERExtQR6RERLJNAjIlpiaC+K7nfyxdx+99oZt5u/5MIp39thu6248qQX9bOsiIihNbSBfvvda7nx1BfP6jOmC/uIiLZJl0tEREsk0CMiWmJou1y232sJ+5y90cORNvEzAGbXbRMRMSqGNtDvWHlq+tAjIjZBulwiIlpiaM/QYfZn2Dtst1WfKomIGH5DG+i9dLfMX3LhrLtlIiLaIl0uEREtMbRn6N10mz5Xp224nBkXI+LBaqQCPWEdETG1dLlERLREAj0ioiUS6BERLZFAj4hoiQR6RERLJNAjIloigR4R0RI9BbqkgyVdL2mVpI3mtJX0HEmXS1on6Yj+lxkRETOZMdAlzQHOAA4BFgKLJC2ctNkvgKOBz/a7wIiI6E0vd4oeAKyyfQOApHOAw4BrJzawfWP93v0DqDEiInrQS5fLLsDNHcur63URETFEegn0jWfEgs2aVEXSYknjksbXrFmzOR8RERFT6CXQVwPzOpZ3BW7ZnMZsn2l7zPbY3LlzN+cjIiJiCr0E+nJggaQ9JG0NHAUsHWxZERGxqWYMdNvrgGOAi4CVwHm2V0g6RdKhAJKeKmk1cCTwUUkrBll0RMQokDTjTz/1NB+67WXAsknrTux4vZyqKyYiImqTn+Ew6Mdm5k7RiIiWSKBHRLREAj0ioiVG6pmiERHDbL+TL+b2u9dOu838JRdO+/4O223FlSe9aLPaT6BHRPTJ7XevnfVFz5kCfzrpcomIaIkEekRES6TLJaJFerlRZfLY6GiPBHpEi5S+kSU2tP1eS9jn7I2eAbSJnwGweb+zBHrEiOplRAVMf5FtNiMqYmN3rDy10YuiCfSIEXX//Lez/Ww/A4CrZ19MDIUEesSIumPlqbP+jB2226oPlcSwSKCPqFz8im5f7fN30bzZdJnA7A6yauqXOzY25vHx8UbaHjW99pVOJ32lEc3rx0VqSZfZHuv2Xs7QR0D6SodfzoxjGCTQR0DTV85jY5O/Ne1+/AUz/jeTfwf51hT9lkAfEU32y03Y5+x9Zv0ZAFe/tr/fFJo4O863puhFt79Nnbbhcj//NtOH3hIlbiDpRxuz/YxcT4gHu/Sht1DpI/+wyJnx8Mv1hOYk0EdUU/8gmu76ydjr4ZfpB5qTLpfom6b+4eaMsFnpBisrXS4xEMPS7ZOwblbTD3WI9RLosdkSpBHDJYEeEbPS9JSxsV4CPSJmZfJ9Bbmm0ZyeHkEn6WBJ10taJWmjQ7GkbSSdW7//Y0nz+11oRIwG2zP+xGDMGOiS5gBnAIcAC4FFkhZO2uz1wG22Hwu8H5h0aSwiIgatlzP0A4BVtm+wfS9wDnDYpG0OA86uX38eeL56+d4VERF900ug7wLc3LG8ul7XdRvb64DbgR37UWBERPSml4ui3c60J3eC9bINkhYDiwF22223HpqOiJjZsE4cV1ovgb4amNexvCtwyxTbrJa0JbAD8LvJH2T7TOBMqO4U3ZyCIyImG/Ug7pdeulyWAwsk7SFpa+AoYOmkbZYCr61fHwF807mUHRFR1Ixn6LbXSToGuAiYA5xle4WkU4Bx20uBTwCflrSK6sz8qEEWHRERG+vpxiLby4Blk9ad2PH6HuDI/pYWERGboqcbiyIiYvgl0CMiWiKBHhHREgn0iIiWaOyJRZLWADfN8mN2Am7tQzmjXgMMRx3DUAMMRx3DUAMMRx3DUAMMRx39qGF323O7vdFYoPeDpPGpHsX0YKphWOoYhhqGpY5hqGFY6hiGGoaljkHXkC6XiIiWSKBHRLTEqAf6mU0XwHDUAMNRxzDUAMNRxzDUAMNRxzDUAMNRx0BrGOk+9IiIWG/Uz9AjIqKWQI+IaIkEekRESyTQIyJaYuQCXdKekrapXz9P0lslPbxwDe+un8w0sfwwSZ8sXMOjJH1C0lfr5YWSXl+yho5aHi3pUEkvkfToJmqo69hF0jMlPWfip3D7kvRqSSfWy7tJOqBkDXW7j5P0DUnX1Mv7SnpXwfZPl/SmLuvfJum0gnXsL+kzki6vf86UtKB+r6epwwdQ046SXibpKYP4/JELdOALwH2SHkv1YI09gM8WrmFL4Mf1P5QXUT3V6bLCNXyK6qEjf1Ev/z/g7wrXgKQ3AJcCL6d6WtUlkv6mgTpOA34AvAt4R/3z94XL+DDwDGBRvXwHcEbhGgA+BpwArAWwfRVlHzrzl3QfnvcB4MUlCpB0OHA+8A3gaOB1wI+A8yU9g+rfTok6LpC0d/16Z+Aa4G+oHgjU/3+vtkfqB7i8/t93AG+pX/+kgTpeANxN9XzVxzbQ/vLJ/9+BKxqo43pgx47lHYHrG6pjm9LtTqph4m+z83dy5YPtbwNYsTnv9bmGq4D5XdbPB+4B3lt6XwD/APxH/Xp74Kp+tzeKZ+hrJS2ieobpBfW6rUoWUH+V/wBwCvBt4EOS/mLa/6j/7pS0I+C6pqcDtxeuAaoHhN/RsXwHcHMDddxA4b+DLtZKmsP638lc4P4G6rhV0p4ddRwB/Kpg+3dNdG10qtfdXaiGLW3fOHllve4m2/9QqI61Ha+fT/3kN9t3MIC/jUb6kWbpdcCbgPfY/rmkPYD/LFzD+4AjbV8LIOnlwDeBJxSs4Tiqh3PvKekHwFyqLo/SfknV/fRfVAFyGHCppOMAbP/7IBuX9MG63buAKyR9A/jTxPu23zrI9if5P8CXgEdKeg/V76NY33WHN1N1eTxB0i+BnwOvLtj+icBXJf0z67six6i6gUp1C66VtJvtX3SulLQ7HX8fBdws6S1UJz5PBr5W17EdAzgBGek7RSU9Apjnqo+wZLtzbN83ad2Otn9buI4tgccDourmWDvDfzKIGk6a7n3bJw+4/dfO0P7Zg2x/MklPoDoTE/AN2ytLtj+plocAW9Rng6Xb3puqW3TvetU1wPtsX12o/ZcCpwPvpTqoGHgqsAQ43vaXC9XxSKpv8jsDZ9i+uF5/IPAU2+/ra3ujFuiSvg0cSvXt4gpgDfAd28cVrOFRVH8ou9g+WNJC4Bm2P1Gwhpd3WX07cLXt35Sqo1N9gP29G/ijqsPrnokDbd31sY3tuwq1vwVVn+jeM248+FruA/4VOGHidyHpcttPbrayDUn6oO23DPDz9wPeDjyR6gB7DfBvtq8cVJubq1/7YhT70Hew/QeqURWftP0UqguUJX2K6ir5zvVyEyNMXg98HHhV/fMxqm6YH0j660E3LunE+mwUSdtI+ibwM+DXkkr/PqAazbBdx/J2wNdLNW77fuBKSbuVanMaK6j+bV8s6c/rdWqwnqk8a5AfbvtK26+x/RTbT65fbxDmdZfdMOjLvhjFQN+yHv7zCtZfFC1tJ9vnUV/UsL0OuG/6/6Tv7gf2sn247cOBhVR9g08Dji/Q/iupRpZAdYF6C6p+/OdSfXspbVvbf5xYqF//WeEadgZW1GPAl078FK4BYJ3td1Id5L9Xj3kera/i5Qz0oFLaKF4UPYXq7Pj7tpdLegzw08I1DMMIk/m2f92x/BvgcbZ/J6lEX/q9HV0r/wP4XN3dsbKhmzbulPRk25cD1CFWakTFhIFeL9gEArB9nqQVwOeAYfjmEAM2coFu+3yqGwYmlm8ADi9cxjCMMPmepAtYvy8OB75b9yX/vkD7f6ovfP0aOJANb+IpfWYMcCzVTSO31Ms7U32LKMb2d0q2N403TLywvULSs4GXNljPVIaxG6gpfdkXIxfokral6j9+IrDtxHrbA787UdJTgZttXy7pucAbqYL0YqphSSW9meo6wrPr5UuBnW3fSRWwg3Ys8Hmqg9n7bf8cQNL/BH5SoP0H1Bckt6YaNjox6ue60qN+6m9qHwT2quuZA9xp+2GF2j/I9jeB3evheZ3+2O2/adgHmi6A4Tmo9GVfjOIol/OB64C/oup+eRWw0vaxBdq+HHhB3a3xHOAc4C3Ak6j6s4uepUt6EtV+eAXVWOMv2P5QyRqGhaQf2X5GwzWMU91ifz7VuOvXAAtK3cQi6WTbJ6n7vEIucdJT1/EVpumzt31oiTp6Ielo258a4OcX3RejGOg/sb2/pKts7ytpK+Ai2wcVaPtK2/vVr88A1tj+p3r5CttPKlDD46hCYxHwW+Bc4O9tTz4jK6K+lnAS1TcFA98HTmlgTP7JVLd7f7GJYZN1DeO2xyb+Nut1P7T9zCbqaUr97RWqb5CPZv2Nf4uAG0sc4IbloFJ6X4xclwvrb6X9fd2H+99U8zOUMEfSlvWolucDizveK7UvrwO+B7zE9iqoZrEr1HY35wDfZf11jFdRHWRKD108DngIsE7SPVRfpV2qu6N2l6Stqe5YPZ3qdvuHlGpc0kuoxsLfVC+fSPV7uQk4dqJbbNAmriVIerftzhkvvyLpuyVqoLqbG6YI0kI1FN8XoxjoZ9Y3sPwj1YXJh1LdalzC54DvSLqVagTF9wBUzfxYapTL4VRn6N+S9DWqQG2yH/DPbb+7Y/mf67v0irK9fek2u/hrquGbxwBvA+ZR9oL9e4CnA0j6S6rb/RcB+wMfoRqNVNJcSY+pBy5QT9Mxt0TDQ3JQ6VRkX4xcl0vT6gtfOwMX1xcgJ7pBHjoxZK5QHQ+hGrmwCDgIOBv40sStxQXreB8wDpxXrzoCeKLtaacEGFAtjwAWsOHF8oH/4+02Z0gTJnUJnkU1HcRp9XLxO0UlHUw1p8wN9ar5wBttF5m6tq5hJfDiSUG6zPZepWqo2y2yL0Ym0FVP9jQVD3gSqGFW3w14JPDKEtcS6jbvoOqjFFW3wsSNVXOAPxbu6piYl/1YYFeqKSGeDvyo0LWVB8JS0hfqG72Kk3QV8Eyqicp+Dhxue7x+71rbCxuoaRvWT1p3ne2SE2MNxUGlo5aB74tR6nIZhq/UQ8n274CP1j+l2hy238exVJMvXWL7wHpaglI3+nR2eT2mUJvd/G+qg9kfqEZ+TYT5/pSdPpe63T+juraxu+2/lbRA0uNtF7vD2/bXVE3b29hBBcrti5EJdA941r7YNJKeYPs6SV2/xpfsfqrdY/seSUjapq7t8YXa9hSvi7J9lqSLgEcCnXOW/DfVtNOlfZJqpsOJ4aSrqYZ0Fgv0YTio1Irsi5EJ9AmSzqa6Yv/7evkRVDOoFX/s2YPccVSjfP6tY11nmBXp+umwWtWzZb8M/F9Jt1E9TaqE/ST9gepMfbv6NTQw0sb2LyV9ADhL0tds32+7+Nl5bU/br1T1QBps3y2p9AX8xg8qtSL7YuQCHdh3IswBbN9Wf6WMsj4u6dG2D4QH5iU/nGpI2D+VLsb2y+qX/yTpW8AO1A8TKND2nBLtbIKPUJ2Rf7C+Ee9Ttq9roI57VT3IYWLOoz0p+3AJGI6DChTaF6M42+IW9Vk58MAFwVE8MI26jwD3AhOP5PsXqpE2t9P9AcEDIWlbSX8n6UOS3ljfJ/Ad20tt31uqjmFi++u2X0X1hJwbqb6x/FDS6+ob8Uo5ieqgOk/SZ6imOH5nwfZhOA4qUGhfjMwolwmSXkP1sNXzqX5Jr6B6HN2nGy3sQWYY7pqt2zqX6maz7wGHUD0vcuDTQAy7+g7eV1ONjb8F+AzV3bz72H5e4TqeTtX9dIntW0u1Xbf/QqrHAC6kmnPpWcDRtr9dso66loHvi5ELdABVTwg6CB54zNe1DZf0oCPpGuBJttdJug5YPDHmW9I1LvTkHklX296nfr0lcGnp8dbDRtIXqUZ1fJqqu+VXHe+N2x4rVMcptk/sWN4C+HT97aGYpg8qdQ1F9sXIdFWommXxTcBjgauBj9S34EczhuGuWeh4qnp9cCnY9ND6kKtZFzdSKsxru0k6wfa/1GOwzweKjn7qCNIL6+UtJH2m9EGFQvtiZM7Qu3y1vtF26ce+RYdhuGtW1fMz75xYpHr03F00M5dLo9T9ObMPsP3FUrUA1BcfP0N1AnYg8FXb7y9cw6eo7pjdIEgnugcL1lFkX4xSoOerdcQ01H3a3AkuNbR30r0JW1Hd8PYD4BN1ISWnyGj0oFJ6X4xSoG8wF8Xk5YgYDvWw0am40HQMQ3FQKb0vRinQ89U6YhqSXm37PzXFvEcl5zuqL/odafvcUm1Oar/xg0pHLcX2xchcFB3Cmzcihs3E3OuNz7Nj+35Jb6aaG7+J9g9s+qDSUUuxfTEyZ+gRMVok/SPVCKhzWf/temIyuVI1fNcbzofeiFL7IoEe0TKq5vx+C9VUsQ98C3fhZ3lK6vaEJNsuNiPlMBxU6jqK7IsEekTLSLqS6uLf1cD9E+tdP8XnwWQYDiolJdAjWkbSj20/rek6AFQ993chGz5F6j+aq6g5JfZFAj2iZST9FdWj+C6mYyKq0nPUSzoJeB5ViC2juiHw+7aPKFxH4weVUvtiZEa5RETP9qGalOsg1ne5mPJz1B8B7Af8xPbrJD0K+HjJAqYKUqD0t4Qi+yKBHtE+LwMeMwTTB99dD9lbJ+lhwG8o/4i+xg8qtSL7IoEe0T5XAg+nCo0mjat6itTHqJ4a9Efg0sI1DMNBBQrti/ShR7SMpG8D+wLL2bAPveiwxUk1zQceZvuqwu1+mOr5CUcBb6cK0itsN/GM1Yma5jOgfZFAj2gZSc/ttr6JYYv1DJDPpurD/77tL5WuoaOW+TRwUOlof+D7IoEeEQNRnx0/lmrufIBXAj+z/ebCdTR+UCm1LxLoES1Tz1P/QWAvYGtgDnBn6QnsJK0A9nYdMvXcKlfbfmLBGobloFJkX+SiaET7fIiqz/h8YAx4DdW49NKuB3YDbqqX5wGluzuey4ZBejbVHbSlFdkXCfSIFrK9StIc2/cBn5T0w1JtS/oKVffGDsBKSZfWy08DitVRa/SgUnpfJNAj2ucuSVsDV0g6HfgV66fWLeF9BdvqaogOKkX3RfrQI1pG0u7Ar6n6z99GFWoftr2qoXoexoazPg58psOpRvp01NDIRGWD3hcJ9IiWkLSb7V80XccESYuBd1NNX3s/658uVvzGniYOKpPaL7IvEugRLdH5nF1JX7B9eMP1/BR4hu1bG6xhKA4qpfZF+tAj2kMdr4dhvu+fUT33t0nvAJ7Y5EGlVmRfJNAj2sNTvG7KCcAPJf2YDacgeGvBGobhoAKF9kUCPaI99pP0B6oz9e3q17C+m6HojUXAR4FvMunJSYUNw0EFCu2LBHpES9ie03QNk6yzfVzDNQzDQQUK7YtcFI2IgZD0Hqober7ChmfHxUaYSPqh7WeWam+aOorsiwR6RAzEMDygeRgOKnUdRfZFAj0iWmsYDiolbdF0ARHRLpLe2fH6yEnvvbdkLbb36PJT8htC0X2RQI+Ifjuq4/UJk947uEQBQ3RQKbovEugR0W+a4nW35UFp/KBSK7ovEugR0W/T3eBU6qLdMBxUoPC+yDj0iOi36W5w2rZQDcNwUIHC+yKjXCKidSTdB9xJHaSsv/1fwLa2t2qqtkFKoEdEtET60CMiWiKBHhHREgn0iIiWSKBHRLREAj0ioiX+P7+1qXRsMeSrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = study.best_trial.params\n",
    "\n",
    "def main():\n",
    "    df_train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    train_y = df_train[\"Survived\"]\n",
    "    train_x = df_train.drop(\"Survived\", axis=1)\n",
    "\n",
    "    train_x = preprocess_df(train_x)\n",
    "    accuracies, feature_importances = validate(train_x, train_y, params) # paramsに書き換えました。\n",
    "    print(np.mean(accuracies))\n",
    "    plot_feature_importances(feature_importances, train_x.columns)\n",
    "\n",
    "    flag_product = True\n",
    "    if flag_product:\n",
    "        df_test = pd.read_csv(\"test.csv\")\n",
    "        df_test_raw = df_test.copy()\n",
    "        test_x = preprocess_df(df_test)\n",
    "        predict_df(train_x, train_y, test_x, df_test_raw, \"result.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## アンサンブルによる精度向上\n",
    "\n",
    "機械学習において、**単一のモデルをそのまま使うのではなく、複数のモデルを組み合わせることで、精度を上げる**手法をアンサンブル学習といいます。\n",
    "\n",
    "実際のKaggleではアンサンブルによる精度向上がかなり大きく、これだけである程度の順位を取ることができます。\n",
    "\n",
    "ここでは、機械学習のアルゴリズムを2つ組み合わせて（LightGBMとXGBoost）、精度がよくなるかを見てみます。\n",
    "\n",
    " - ソースコード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8327721661054994\n"
     ]
    }
   ],
   "source": [
    "# 今回はクロスバリデーションで精度を出す以外のところは削っています。\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# main文\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "train_y = df_train[\"Survived\"]\n",
    "train_x = df_train.drop(\"Survived\", axis=1)\n",
    "\n",
    "train_x = preprocess_df(train_x)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "for train_idx, test_idx in cv.split(train_x, train_y):\n",
    "    trn_x = train_x.iloc[train_idx, :]\n",
    "    val_x = train_x.iloc[test_idx, :]\n",
    "\n",
    "    trn_y = train_y.iloc[train_idx]\n",
    "    val_y = train_y.iloc[test_idx]\n",
    "\n",
    "    clf_xgb = xgb.XGBClassifier(**params)\n",
    "    clf_lgb = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    clf_xgb.fit(trn_x, trn_y)\n",
    "    clf_lgb.fit(trn_x, trn_y)\n",
    "    \n",
    "    # 平均値化するためにprobabilityを出した\n",
    "    pred_proba_y_xgb = clf_xgb.predict_proba(val_x)[:, 1]\n",
    "    pred_proba_y_lgb = clf_lgb.predict_proba(val_x)[:, 1]\n",
    "    \n",
    "    # probabilityの平均値が0.50を超えていれば1, そうでないなら0\n",
    "    pred_proba_y = pd.DataFrame({\"xgb\": pred_proba_y_xgb, \"lgb\": pred_proba_y_lgb}).mean(axis=1)\n",
    "    pred_y = [1 if proba > 0.50 else 0 for proba in pred_proba_y]\n",
    "    accuracies.append(accuracy_score(val_y, pred_y))\n",
    "\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "XGBoost単体より少し精度が上がっていることがわかると思います。\n",
    "\n",
    "今回は単純に平均値を取っただけですが、実際は複数の判別モデルから出力された値をさらに機械学習に入れ込むStackingなど、\n",
    "\n",
    "様々な手法が取られています。\n",
    "\n",
    "## おわりに\n",
    "\n",
    "機械学習において、特徴量エンジニアリングの方がハイパーパラメーター探索より重要であることが多いです。\n",
    "\n",
    "ただ、ハイパーパラメーターのチューニングも必要な技術ではあるので、一通りやってみました。\n",
    "\n",
    "ここで概ね機械学習の講座は終わりです。 \n",
    "\n",
    "ただ、まだまだ入り口であり、やることはたくさんあります。\n",
    "\n",
    "ここでは、この次にやった方がよさそうなことをつらつらと書いていきます。\n",
    "\n",
    "目的によって優先度は変わるので、ご自身で優先度を決めていただければと思いますが、\n",
    "\n",
    "強いていうならばとりあえずコンペ参加がよいのではないでしょうか。\n",
    "\n",
    "---\n",
    "\n",
    "## 実際のコンペへ参加\n",
    "\n",
    "実際のコンペへ参加し、Kernelを読み、discussionを読むということが総合的には１番勉強になります。\n",
    "\n",
    "[KaggleのCompetitionsのページ](https://www.kaggle.com/competitions)から自分が向いてそうなコンペを探してみてください。\n",
    "\n",
    "また、特徴量エンジニアリングという意味では最近でた本もオススメです。\n",
    "\n",
    "なお、Kaggle以外のコンペサイトはKernelの共有などがないため最初は難しいと思います。。\n",
    "\n",
    "参考：[Kaggle - Competitions](https://www.kaggle.com/competitions)\n",
    "\n",
    "参考：[機械学習のための特徴量エンジニアリング](https://www.amazon.co.jp/dp/4873118689/)\n",
    "\n",
    "---\n",
    "\n",
    "## 機械学習アルゴリズムの把握\n",
    "\n",
    "今回は流れの説明を優先したため、機械学習アルゴリズムの説明はしませんでした。\n",
    "\n",
    "ただし、他人に説明するときなどはアルゴリズムの内容も把握している必要があるでしょう。\n",
    "\n",
    "機械学習アルゴリズムの詳細は参考に挙げた本が詳細かつわかりやすく説明してくれています。\n",
    "\n",
    "参考：[はじめてのパターン認識](https://www.amazon.co.jp/dp/4627849710)\n",
    "\n",
    "---\n",
    "\n",
    "## Kaggleで戦う環境を整える\n",
    "\n",
    "Kaggleで本格的にコンペに参加するには、効率化を進める方がよいです。\n",
    "\n",
    "### 計算機環境を整える\n",
    "\n",
    "今回はあまり大きくないデータセットであったため、Google Colabでも計算ができましたが、\n",
    "\n",
    "本格的なコンペはこれより１、２桁大きいデータセットであることが多いです。\n",
    "\n",
    "そのようなときは、GCPなどのサーバー環境を借りて計算する方が自由に動けるのでオススメです。\n",
    "\n",
    "参考：[GCPとDockerでKaggle用計算環境構築](https://qiita.com/lain21/items/a33a39d465cd08b662f1)\n",
    "\n",
    "### Kaggle APIの登録\n",
    "\n",
    "コマンドラインからKaggleにSubmitできるAPIを公式が準備してくれています。\n",
    "\n",
    "GCPなどのサーバー環境を使うならば登録する方がベターです。\n",
    "\n",
    "参考：[Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
